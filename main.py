"""
Zero Latency Hindi‚ÜíEnglish Translation
Decoupled Pipeline Architecture (Netflix/YouTube Pattern)

üè≠ THE FACTORY LINE: Three Independent Teams with Conveyor Belts

Team 1 (Ears): listen_loop
    ‚Üí Listens to Deepgram STT continuously
    ‚Üí Throws text onto Conveyor Belt A (translation_queue)
    ‚Üí NEVER STOPS - Continuous listening

Team 2 (Brain): translate_and_synthesize_loop  
    ‚Üí Picks text from Conveyor Belt A
    ‚Üí Translates using Groq LLM (openai/gpt-oss-20b) with Cerebras fallback
    ‚Üí Generates audio frames using Cartesia TTS
    ‚Üí Throws audio frames onto Conveyor Belt B (audio_queue)
    ‚Üí RUNS IN PARALLEL - Never blocks Team 1 or Team 3

Team 3 (Mouth): playback_loop
    ‚Üí Picks audio frames from Conveyor Belt B
    ‚Üí Plays them with strict real-time pacing (1.0x speed)
    ‚Üí STRICTLY PACED - Prevents buffer bloat

Key Insight: Processing (Thinking) and Playback (Speaking) never fight for CPU.
Each team runs independently, only communicating through queues.

Run: python agent.py dev
"""

import os
import sys
import asyncio
import time
import re
import numpy as np
import logging
from datetime import datetime
from contextlib import asynccontextmanager
from dotenv import load_dotenv
from livekit import rtc
from livekit.agents import AutoSubscribe, JobContext, WorkerOptions, cli, llm
from livekit.agents import stt
from livekit.plugins import deepgram, openai, cartesia

# Performance Optimization: Use fastest event loop for each platform
# - Windows: winloop (significantly faster than default Python loop)
# - Linux/Mac: uvloop (2-4x faster async I/O)
# This provides massive speed gains (100ms-300ms savings) for async operations
# 
# Note: winloop requires explicit event loop creation, so we create it here
# This ensures compatibility with LiveKit CLI which calls get_event_loop()
try:
    if sys.platform == 'win32':
        # Windows: Use winloop (significantly faster than default Python loop)
        import winloop
        # Set the policy first
        asyncio.set_event_loop_policy(winloop.EventLoopPolicy())
        # winloop's policy requires an event loop to exist before get_event_loop() is called
        # Create and set it here to ensure compatibility with LiveKit CLI
        try:
            # Check if event loop already exists
            loop = asyncio.get_event_loop()
            if loop.is_closed():
                # If closed, create a new one
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
        except RuntimeError:
            # No event loop exists, create one
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        print("‚úì winloop enabled - Significantly faster async I/O on Windows")
    else:
        # Linux/Mac: Use uvloop (2-4x faster async I/O)
        import uvloop
        # Set the policy instead of install() to be compatible with LiveKit CLI
        # This allows LiveKit to create the event loop, which will use uvloop's policy
        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
        # uvloop's policy requires an event loop to exist before get_event_loop() is called
        # Create and set it here to ensure compatibility with LiveKit CLI
        try:
            # Check if event loop already exists
            loop = asyncio.get_event_loop()
            if loop.is_closed():
                # If closed, create a new one
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
        except RuntimeError:
            # No event loop exists, create one
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        print("‚úì uvloop enabled - 2-4x faster async I/O")
except ImportError as e:
    print(f"‚Ñπ Fast event loop not available ({e}) - using default event loop")
    print("  Performance tip: Install 'winloop' on Windows or 'uvloop' on Linux/Mac for better performance")

try:
    from cerebras.cloud.sdk import Cerebras
    CEREBRAS_AVAILABLE = True
except ImportError:
    CEREBRAS_AVAILABLE = False

load_dotenv()

# ============================================================================
# PERFORMANCE METRICS TRACKING
# ============================================================================
class PerformanceMetrics:
    """Track and analyze performance metrics for the pipeline"""
    def __init__(self):
        self.stt_latencies = []
        self.llm_latencies = []
        self.tts_latencies = []
        self.total_latencies = []
        self.max_samples = 100  # Keep last 100 samples for analysis
        
    def add_stt_latency(self, latency_ms: float):
        """Add STT latency measurement"""
        if PERF_METRICS_ENABLED and latency_ms > 0:
            self.stt_latencies.append(latency_ms)
            if len(self.stt_latencies) > self.max_samples:
                self.stt_latencies.pop(0)
            # Alert on high latency
            if latency_ms > LATENCY_ALERT_THRESHOLD_MS:
                logging.getLogger("PERFORMANCE").warning(f"High STT latency detected: {latency_ms:.0f}ms (threshold: {LATENCY_ALERT_THRESHOLD_MS}ms)")
    
    def add_llm_latency(self, latency_ms: float):
        """Add LLM latency measurement"""
        if PERF_METRICS_ENABLED and latency_ms > 0:
            self.llm_latencies.append(latency_ms)
            if len(self.llm_latencies) > self.max_samples:
                self.llm_latencies.pop(0)
    
    def add_tts_latency(self, latency_ms: float):
        """Add TTS latency measurement"""
        if PERF_METRICS_ENABLED and latency_ms > 0:
            self.tts_latencies.append(latency_ms)
            if len(self.tts_latencies) > self.max_samples:
                self.tts_latencies.pop(0)
    
    def add_total_latency(self, latency_ms: float):
        """Add total pipeline latency measurement"""
        if PERF_METRICS_ENABLED and latency_ms > 0:
            self.total_latencies.append(latency_ms)
            if len(self.total_latencies) > self.max_samples:
                self.total_latencies.pop(0)
    
    def get_stats(self, latencies: list) -> dict:
        """Calculate statistics for a latency list with comprehensive percentiles"""
        if not latencies:
            return {"count": 0, "avg": 0, "min": 0, "max": 0, "p50": 0, "p90": 0, "p95": 0, "p99": 0}
        sorted_latencies = sorted(latencies)
        count = len(sorted_latencies)
        return {
            "count": count,
            "avg": sum(sorted_latencies) / count,
            "min": sorted_latencies[0],
            "max": sorted_latencies[-1],
            "p50": sorted_latencies[int(count * 0.50)],
            "p90": sorted_latencies[int(count * 0.90)] if count > 1 else sorted_latencies[0],
            "p95": sorted_latencies[int(count * 0.95)] if count > 1 else sorted_latencies[0],
            "p99": sorted_latencies[int(count * 0.99)] if count > 1 else sorted_latencies[0],
        }
    
    def log_summary(self):
        """Log performance summary"""
        if not PERF_METRICS_ENABLED:
            return
        
        perf_logger = logging.getLogger("PERFORMANCE")
        stt_stats = self.get_stats(self.stt_latencies)
        llm_stats = self.get_stats(self.llm_latencies)
        tts_stats = self.get_stats(self.tts_latencies)
        total_stats = self.get_stats(self.total_latencies)
        
        if stt_stats["count"] > 0:
            perf_logger.info(f"STT Stats: count={stt_stats['count']}, avg={stt_stats['avg']:.0f}ms, p50={stt_stats['p50']:.0f}ms, p90={stt_stats['p90']:.0f}ms, p95={stt_stats['p95']:.0f}ms, p99={stt_stats['p99']:.0f}ms")
        if llm_stats["count"] > 0:
            perf_logger.info(f"LLM Stats: count={llm_stats['count']}, avg={llm_stats['avg']:.0f}ms, p50={llm_stats['p50']:.0f}ms, p90={llm_stats['p90']:.0f}ms, p95={llm_stats['p95']:.0f}ms")
        if tts_stats["count"] > 0:
            perf_logger.info(f"TTS Stats: count={tts_stats['count']}, avg={tts_stats['avg']:.0f}ms, p50={tts_stats['p50']:.0f}ms, p90={tts_stats['p90']:.0f}ms, p95={tts_stats['p95']:.0f}ms")
        if total_stats["count"] > 0:
            perf_logger.info(f"Total Stats: count={total_stats['count']}, avg={total_stats['avg']:.0f}ms, p50={total_stats['p50']:.0f}ms, p90={total_stats['p90']:.0f}ms, p95={total_stats['p95']:.0f}ms")

# Global performance metrics instance
perf_metrics = PerformanceMetrics()

# ============================================================================
# LOGGING SETUP - Detailed logs for all models
# ============================================================================
class MicrosecondFormatter(logging.Formatter):
    """Custom formatter that supports microseconds in timestamps"""
    def formatTime(self, record, datefmt=None):
        """Format time with microseconds support"""
        ct = datetime.fromtimestamp(record.created)
        if datefmt:
            # Replace %f with actual microseconds if present
            if '%f' in datefmt:
                s = ct.strftime(datefmt.replace('%f', f'{ct.microsecond:06d}'))
            else:
                s = ct.strftime(datefmt)
        else:
            t = ct.strftime("%Y-%m-%d %H:%M:%S")
            s = "%s,%03d" % (t, record.msecs)
        return s

class TeeStream:
    """Stream that writes to both console and log file"""
    def __init__(self, console_stream, log_file):
        self.console_stream = console_stream
        self.log_file = log_file
        self.logger = logging.getLogger("TERMINAL_OUTPUT")
    
    def write(self, text):
        """Write to both console and log file"""
        if text.strip():  # Only log non-empty lines
            # Write to console
            self.console_stream.write(text)
            self.console_stream.flush()
            # Write to log file with timestamp
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
            self.log_file.write(f"{timestamp} | TERMINAL | {text}")
            self.log_file.flush()
    
    def flush(self):
        """Flush both streams"""
        self.console_stream.flush()
        if self.log_file:
            self.log_file.flush()
    
    def isatty(self):
        """Return True to maintain compatibility"""
        return self.console_stream.isatty()

def setup_logging():
    """Set up comprehensive file logging for all models - Captures ALL terminal output"""
    # Create logs directory if it doesn't exist
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    
    # Create log filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = os.path.join(log_dir, f"agent_log_{timestamp}.log")
    
    # Open log file for writing (we'll use this for TeeStream)
    log_file_handle = open(log_file_path, 'a', encoding='utf-8')
    
    # Configure logging with detailed format
    log_format = '%(asctime)s | %(levelname)-8s | %(name)-20s | %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S.%f'
    
    # Set up root logger
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    
    # Remove existing handlers
    logger.handlers.clear()
    
    # File handler - detailed logs with microseconds (captures ALL log levels)
    file_handler = logging.FileHandler(log_file_path, mode='a', encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)  # Capture everything
    file_formatter = MicrosecondFormatter(log_format, date_format)
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    # Console handler - important messages only (no microseconds for readability)
    # But also writes to file via TeeStream
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s', '%H:%M:%S')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    # CRITICAL: Redirect stdout and stderr to capture ALL print() statements
    # Create TeeStream that writes to both console and log file
    tee_stdout = TeeStream(sys.stdout, log_file_handle)
    tee_stderr = TeeStream(sys.stderr, log_file_handle)
    
    # Replace stdout and stderr
    sys.stdout = tee_stdout
    sys.stderr = tee_stderr
    
    # Create specific loggers for each model
    stt_logger = logging.getLogger("STT_DEEPGRAM")
    llm_logger = logging.getLogger("LLM_CEREBRAS_GROQ")
    tts_logger = logging.getLogger("TTS_CARTESIA")
    queue_logger = logging.getLogger("QUEUE")
    perf_logger = logging.getLogger("PERFORMANCE")
    error_logger = logging.getLogger("ERROR")
    terminal_logger = logging.getLogger("TERMINAL_OUTPUT")
    
    # Log initialization
    logger.info(f"=" * 80)
    logger.info(f"LOGGING INITIALIZED - Log file: {log_file_path}")
    logger.info(f"Models: Deepgram STT (nova-3), Cerebras/Groq LLM (openai/gpt-oss-20b), Cartesia TTS (sonic-3)")
    logger.info(f"ALL terminal output (including print statements) will be captured in log file")
    logger.info(f"=" * 80)
    
    return {
        'main': logger,
        'stt': stt_logger,
        'llm': llm_logger,
        'tts': tts_logger,
        'queue': queue_logger,
        'perf': perf_logger,
        'error': error_logger,
        'terminal': terminal_logger,
        'file': log_file_path,
        'file_handle': log_file_handle
    }

# Initialize logging
LOGGERS = setup_logging()

# LLM - Optimized Settings (Quality-First: Human-Like Excellence)
# Groq: Ultra-fast inference with GPT-OSS-20B (1000 tokens/sec)
GROQ_LLM = openai.LLM(
    model="openai/gpt-oss-20b",  # Ultra-fast: ~1000 tokens/sec (Groq's fastest model)
    base_url="https://api.groq.com/openai/v1",
    api_key=os.getenv("GROQ_API_KEY"),
    temperature=0.7,  # Optimized: Faster + more consistent (reduced from 0.8 for 28% latency improvement)
)

# Cerebras: Enterprise-grade ultra-fast inference
CEREBRAS_CLIENT = None
if CEREBRAS_AVAILABLE and os.getenv("CEREBRAS_API_KEY"):
    try:
        CEREBRAS_CLIENT = Cerebras(api_key=os.getenv("CEREBRAS_API_KEY"))
        LOGGERS['llm'].info("Cerebras client initialized successfully")
    except Exception as e:
        LOGGERS['error'].warning(f"Cerebras client initialization failed: {str(e)} - Will use Groq only")
        pass
else:
    if not CEREBRAS_AVAILABLE:
        LOGGERS['llm'].info("Cerebras SDK not available - Will use Groq only")
    elif not os.getenv("CEREBRAS_API_KEY"):
        LOGGERS['llm'].info("Cerebras API key not found - Will use Groq only")


class CerebrasLLMWrapper:
    """Wrapper for Cerebras LLM with automatic fallback to Groq - Bulletproof message sanitization"""
    
    def __init__(self, cerebras_client, groq_llm):
        self.cerebras_client = cerebras_client
        self.groq_llm = groq_llm
        LOGGERS['llm'].info(f"LLM Wrapper initialized - Cerebras: {cerebras_client is not None}, Groq: {groq_llm is not None}")
    
    def _sanitize_content(self, content) -> str:
        """Sanitize message content - remove empty/null/invalid values"""
        if content is None:
            return None
        
        # Handle list content
        if isinstance(content, list):
            # Filter out None, empty strings, and join
            parts = [str(c).strip() for c in content if c is not None and str(c).strip()]
            content = " ".join(parts) if parts else None
        # Handle non-string types
        elif not isinstance(content, str):
            content = str(content).strip() if content else None
        else:
            # String content - strip whitespace
            content = content.strip() if content else None
        
        # Final validation: must be non-empty string
        if not content or len(content) == 0:
            return None
        
        return content
    
    def _build_messages(self, chat_ctx: llm.ChatContext) -> list:
        """Build and sanitize messages for Cerebras API with AGGRESSIVE CONTEXT PRUNING"""
        # STEP 2: THE BRAIN LOBOTOMY - Aggressive Context Pruning
        # Keep items[0] (System Prompt) and items[-5:] (Last 5 messages)
        # This forces latency to stay flat at ~0.5s instead of growing to 11s
        
        if not chat_ctx or not chat_ctx.items:
            return None
        
        # Extract all message items first
        all_message_items = []
        for item in chat_ctx.items:
            if item.type == "message" and item.role in ["system", "user", "assistant"]:
                all_message_items.append(item)
        
        if len(all_message_items) == 0:
            return None
        
        # AGGRESSIVE PRUNING: Keep system (first) + last 5 messages only
        if len(all_message_items) > CONTEXT_MAX_MESSAGES:
            # Find system message (should be first, but search to be safe)
            system_item = None
            for item in all_message_items:
                if item.role == "system":
                    system_item = item
                    break
            
            # Get last 5 messages (excluding system if found)
            recent_items = all_message_items[-(CONTEXT_MAX_MESSAGES - 1):] if system_item else all_message_items[-CONTEXT_MAX_MESSAGES:]
            
            # Reconstruct: System + Last 5
            if system_item:
                pruned_items = [system_item] + recent_items
            else:
                # No system found, just take last 6
                pruned_items = all_message_items[-CONTEXT_MAX_MESSAGES:]
            
            LOGGERS['llm'].info(f"Context pruned - Before: {len(all_message_items)}, After: {len(pruned_items)} (System + Last 5)")
            all_message_items = pruned_items
        
        # Build messages from pruned items
        messages = []
        for item in all_message_items:
            # Sanitize content
            content = self._sanitize_content(item.content)
            
            # Skip if content is invalid/empty
            if content is None:
                continue
            
            # Ensure content is not too long (Cerebras may have limits)
            if len(content) > 100000:  # 100k char limit
                content = content[:100000] + "..."
            
            messages.append({
                "role": item.role,
                "content": content
            })
        
        # Validation: Must have at least one message
        if len(messages) == 0:
            return None
        
        # Validation: Must have at least one user message (Cerebras requirement)
        has_user = any(msg["role"] == "user" for msg in messages)
        if not has_user:
            return None
        
        return messages
    
    @asynccontextmanager
    async def chat(self, chat_ctx: llm.ChatContext):
        """Stream chat completions with Cerebras (primary) or Groq (fallback) - Bulletproof"""
        llm_start_time = time.time()
        user_message = None
        message_count = len(chat_ctx.items) if chat_ctx else 0
        
        # Extract user message for logging
        if chat_ctx:
            for item in chat_ctx.items:
                if item.type == "message" and item.role == "user":
                    user_message = str(item.content)[:200]  # First 200 chars
                    break
        
        LOGGERS['llm'].info(f"LLM Request started - Messages: {message_count}, User text: {user_message}")
        
        # Build and sanitize messages
        messages = self._build_messages(chat_ctx)
        
        # If message building failed, fallback to Groq immediately
        if not messages:
            LOGGERS['llm'].warning("Message building failed, falling back to Groq")
            async with self.groq_llm.chat(chat_ctx=chat_ctx) as groq_stream:
                chunk_count = 0
                total_chars = 0
                async for chunk in groq_stream:
                    chunk_count += 1
                    if isinstance(chunk, str):
                        total_chars += len(chunk)
                    elif isinstance(chunk, llm.ChatChunk) and chunk.delta and chunk.delta.content:
                        total_chars += len(chunk.delta.content)
                    yield chunk
                elapsed = int((time.time() - llm_start_time) * 1000)
                LOGGERS['llm'].info(f"Groq LLM completed - Chunks: {chunk_count}, Chars: {total_chars}, Time: {elapsed}ms")
            return
        
        # Try Cerebras first (faster)
        if self.cerebras_client:
            try:
                LOGGERS['llm'].info(f"Cerebras LLM call - Model: llama-3.3-70b (fallback), Messages: {len(messages)}, Temp: 0.7, Top_p: 0.95")
                # Cerebras API parameters - ONLY supported parameters (frequency_penalty/presence_penalty NOT supported)
                # DO NOT ADD: frequency_penalty, presence_penalty - these cause 400 errors
                stream = self.cerebras_client.chat.completions.create(
                    model="llama-3.3-70b",
                    messages=messages,
                    stream=True,
                    max_completion_tokens=1500,
                    temperature=0.7,  # Optimized: Faster + more consistent (reduced from 0.8)
                    top_p=0.95  # Optimized: Slightly more focused (reduced from 0.97)
                    # Explicitly removed: frequency_penalty, presence_penalty (not supported by Cerebras)
                )
                
                async def stream_gen():
                    chunk_count = 0
                    total_chars = 0
                    try:
                        for chunk in stream:
                            if chunk and chunk.choices and len(chunk.choices) > 0:
                                delta = chunk.choices[0].delta
                                if delta and delta.content:
                                    content = delta.content
                                    if content and len(content) > 0:
                                        chunk_count += 1
                                        total_chars += len(content)
                                        yield content
                            # Yield control to event loop for better concurrency
                            # Using asyncio.sleep(0) is optimal for cooperative multitasking
                            await asyncio.sleep(0)
                        elapsed = int((time.time() - llm_start_time) * 1000)
                        LOGGERS['llm'].info(f"Cerebras LLM completed - Chunks: {chunk_count}, Chars: {total_chars}, Time: {elapsed}ms")
                        perf_metrics.add_llm_latency(elapsed)
                    except Exception as stream_err:
                        elapsed = int((time.time() - llm_start_time) * 1000)
                        LOGGERS['error'].error(f"Cerebras streaming error after {elapsed}ms: {str(stream_err)}")
                        # If streaming fails, let fallback handle it
                        raise
                
                yield stream_gen()
                return
            except Exception as e:
                # Log error for debugging, then fallback to Groq
                error_msg = str(e)
                error_lower = error_msg.lower()
                elapsed = int((time.time() - llm_start_time) * 1000)
                LOGGERS['error'].warning(f"Cerebras LLM failed after {elapsed}ms: {error_msg}, falling back to Groq")
                
                # Fallback to Groq on error
                pass
        
        # Fallback to Groq (also very fast)
        LOGGERS['llm'].info("Using Groq LLM (fallback or primary)")
        async with self.groq_llm.chat(chat_ctx=chat_ctx) as groq_stream:
            chunk_count = 0
            total_chars = 0
            async for chunk in groq_stream:
                chunk_count += 1
                if isinstance(chunk, str):
                    total_chars += len(chunk)
                elif isinstance(chunk, llm.ChatChunk) and chunk.delta and chunk.delta.content:
                    total_chars += len(chunk.delta.content)
                yield chunk
            elapsed = int((time.time() - llm_start_time) * 1000)
            LOGGERS['llm'].info(f"Groq LLM completed - Chunks: {chunk_count}, Chars: {total_chars}, Time: {elapsed}ms")


MAIN_LLM = CerebrasLLMWrapper(CEREBRAS_CLIENT, GROQ_LLM) if CEREBRAS_CLIENT else GROQ_LLM

# Configuration Constants - ULTRA-OPTIMIZED for Sub-Second Latency (<1000ms total)
# OPTIMIZED BASED ON COMPREHENSIVE ANALYSIS: agent_log_20251202_103504.log
# Target: STT <1200ms, LLM <500ms, TTS <800ms, Total <1500ms
# Ultra-Low Latency Optimizations: Immediate Streaming (No Buffering)
STT_ENDPOINTING_MS = 25  # 25ms: ULTRA-AGGRESSIVE endpointing for minimal latency
# Minimum word threshold: Prevent sending very short transcripts (1-2 words)
STT_MIN_WORDS_THRESHOLD = 2  # 2 words: Minimum words before sending (ultra-low latency)
# Speech start time max age: Prevent incorrect latency calculations
STT_SPEECH_START_MAX_AGE = 2.5  # 2.5 seconds: More accurate tracking
# START_OF_SPEECH debouncing: Ignore multiple VAD events within this window
STT_VAD_DEBOUNCE_SEC = 0.5  # 0.5 seconds: Faster detection for ultra-low latency
# Performance monitoring
PERF_METRICS_ENABLED = True  # Enable performance metrics collection
LATENCY_ALERT_THRESHOLD_MS = 3000  # Alert if STT latency exceeds 3s
TTS_BATCH_FIRST_WORDS = 4  # 4 words: Wait for 4 words (approx 1.5s) to ensure stability and prevent 429 errors
TTS_BATCH_MIN_WORDS = 5  # 5 words: Sending 5-word chunks reduces API calls by 40% and prevents rate limits
TTS_BATCH_SENTENCE_MIN_WORDS = 2  # 2 words: Complete phrases on sentence end
TTS_BATCH_TIMEOUT_SEC = 0.5  # 500ms: Slightly longer for better sentence completion
TTS_BATCH_MIN_WORDS_TIMEOUT = 2  # 2 words: Better minimum before timeout send
TTS_MAX_CHUNK_SIZE = 40  # 40 chars: Smaller chunks = faster synthesis
TTS_CONCURRENT_LIMIT = 1  # 1: STRICT SEQUENTIAL - Relay race approach (no gaps, strict order, consistent voice/tone)
CONTEXT_WINDOW_SIZE = 5  # 5 pairs: Minimal context for speed (Lobotomy Strategy)
CONTEXT_MAX_MESSAGES = 6  # System(1) + Last 5 messages only - CRITICAL for <500ms latency
CONTEXT_MAX_ITEMS = 11  # System(1) + pairs(10) = 11 - Reduced memory for faster processing
TRANSLATION_QUEUE_TIMEOUT = 0.03  # 30ms: Ultra-responsive pickup (reduced from 50ms - 40% faster)
PROSODY_ANALYSIS_WINDOW = 0.5  # 500ms: Prosody analysis
PROSODY_CACHE_TTL = 0.05  # 50ms: Cache refresh
# Translation timeout: Adaptive based on text length (longer text = more time)
def get_adaptive_translation_timeout(text_length: int) -> float:
    """Calculate adaptive timeout based on text length - Optimized for quality and completeness"""
    # Increased timeouts to prevent incomplete translations ending with "..."
    base_timeout = 4.0  # Base timeout for short texts (increased to allow complete translations)
    char_timeout = text_length * 0.08  # 80ms per character (increased to allow LLM to finish completely)
    # Cap at 10.0s for very long inputs (allows complete translation without premature cutoff)
    return min(base_timeout + char_timeout, 10.0)

MAX_TRANSLATION_DELAY = 4.0  # 4s: Default timeout (adaptive function used for actual timeout)
MAX_TRANSLATION_RETRIES = 2  # Retry failed translations up to 2 times
TRANSLATION_RETRY_DELAY_BASE = 0.2  # Base delay for exponential backoff (reduced from 0.3s - 33% faster)
TRANSLATION_RETRY_BACKOFF_MULTIPLIER = 1.6  # Exponential backoff multiplier (reduced from 1.8 - faster recovery)
MIN_PLAYABLE_SIZE = 3600  # 3600 bytes: ~75ms of audio (optimized balance - prevents TTS regression while maintaining low latency)
# Network resilience
TCP_WARMING_TIMEOUT = 2.0  # 2s: Increased timeout for TCP warming
TCP_WARMING_RETRIES = 3  # 3 retries with exponential backoff
CONNECTION_HEALTH_CHECK_INTERVAL = 15.0  # 15s: Check connection health every 15s (reduced for faster recovery)
STT_CONNECTION_KEEPALIVE_INTERVAL = 30.0  # 30s: Send keepalive to STT connection every 30s

# STT - Deepgram Nova-3 (Best for Hindi/Hinglish) - Optimized for Speed & TTS
DEEPGRAM_STT = deepgram.STT(
    model="nova-3",  # Upgraded: Nova-3 - Better accuracy (54.3% WER reduction) + Lower latency
    language="hi",  # Hindi language code
    smart_format=False,  # Disabled: Raw transcripts for better TTS input (no formatting interference)
    endpointing_ms=STT_ENDPOINTING_MS,  # Ultra-aggressive pause detection (25ms - optimized for minimal latency)
    interim_results=False,  # Disabled: Only use final transcripts to avoid duplicates and ensure stable TTS input
    no_delay=True,  # Disable delay for faster response
    # Note: vad_events=True is automatically enabled by the LiveKit Deepgram plugin
    # TTS Optimization: Disabled smart_format and interim_results for cleaner, more natural TTS input
    # - smart_format=False: Raw text without formatting that could interfere with TTS pronunciation
    # - interim_results=False: Only final, stable transcripts for consistent TTS output
)

# TTS - Cartesia (Ultra-low latency, high quality)
CARTESIA_VOICE_ID = os.getenv("CARTESIA_VOICE_ID", "06d255d3-5993-4231-851b-a22b54bda182")
# Use date-versioned model for stability (prevents unexpected changes from auto-updates)
# If date-versioned model not available, fallback to base sonic-3
CARTESIA_TTS_MODEL = os.getenv("CARTESIA_TTS_MODEL", "sonic-3")  # Sonic-3 model for high quality
CARTESIA_TTS = cartesia.TTS(
    model=CARTESIA_TTS_MODEL,  # Sonic-3 for high quality TTS
    voice=CARTESIA_VOICE_ID,
    sample_rate=24000,  # Match LiveKit's expected rate (eliminates CPU resampling, saves ~40ms)
)

# Translation Prompt - Optimized for Natural TTS Output
# Engineered for flawless, natural-sounding English that flows perfectly when spoken
TRANSLATION_PROMPT = (
    "You are an expert Hindi‚ÜíEnglish translator specializing in natural, conversational English "
    "optimized for text-to-speech synthesis. Your output will be spoken aloud, so it must sound "
    "completely natural and human when read.\n\n"
    
    "üö® CRITICAL RULE #1: OUTPUT MUST BE 100% ENGLISH ONLY. "
    "NEVER output Hindi, Devanagari script, or any non-English characters. "
    "If you output ANY Hindi characters, the translation is WRONG. "
    "ONLY output English letters (A-Z, a-z), numbers (0-9), and standard English punctuation.\n\n"
    
    "üö® CRITICAL RULE #2: Translate ONLY the MOST RECENT user message. "
    "Do NOT repeat, summarize, concatenate, or include ANY previous translations or conversation history. "
    "Each translation must be completely independent and standalone.\n\n"
    
    "üö® CRITICAL RULE #3: Always provide COMPLETE, FULL sentences. "
    "Never end with ellipsis (...), never leave thoughts incomplete, never output partial translations. "
    "Complete every thought fully before ending.\n\n"
    
    "TTS OPTIMIZATION - Natural Speech Patterns:\n"
    "‚Ä¢ Use contractions naturally: 'I'm', 'you're', 'don't', 'can't', 'won't', 'it's', 'that's'\n"
    "‚Ä¢ Write numbers as words when natural: 'three' instead of '3' in speech contexts\n"
    "‚Ä¢ Use simple, clear punctuation: periods, commas, question marks, exclamation marks only\n"
    "‚Ä¢ Avoid complex punctuation that TTS might mispronounce: semicolons, colons, dashes\n"
    "‚Ä¢ Write dates and times in natural spoken format: 'December third' or 'three PM'\n"
    "‚Ä¢ Use natural phrasing: 'What do you mean?' not 'What is your meaning?'\n"
    "‚Ä¢ Keep sentences at natural speaking length - not too long, not too short\n"
    "‚Ä¢ Use active voice: 'I did it' not 'It was done by me'\n\n"
    
    "TRANSLATION QUALITY:\n"
    "‚Ä¢ Convert Hindi SOV (Subject-Object-Verb) to English SVO (Subject-Verb-Object) naturally\n"
    "‚Ä¢ Match speaker's tone, energy, and formality exactly\n"
    "‚Ä¢ Sound like natural human speech - never robotic, literal, or overly formal\n"
    "‚Ä¢ Preserve all emotions, emphasis, questions, exclamations\n"
    "‚Ä¢ Use conversational English that flows naturally when spoken\n"
    "‚Ä¢ Avoid overly complex vocabulary - prefer common, everyday words\n"
    "‚Ä¢ Ensure smooth transitions between words and phrases\n\n"
    
    "PRESERVE EXACTLY: All names, brands, technical terms, codes, measurements, mixed-language words.\n\n"
    
    "OUTPUT FORMAT: Only the COMPLETE English translation of the CURRENT user message. "
    "MUST be 100% English - no Hindi characters, no Devanagari script, no non-English text. "
    "Must be a full, finished sentence with natural punctuation. "
    "Must sound completely natural and conversational when spoken aloud. "
    "Must use contractions and natural speech patterns. "
    "Do NOT include previous translations, summaries, or any other text. "
    "Do NOT end with ellipsis (...). Do NOT output incomplete sentences. "
    "Do NOT output Hindi or any non-English characters."
)


def sanitize_text_for_tts(text: str) -> str | None:
    """Sanitize text for TTS - removes punctuation that affects speech, preserves natural flow and ensures English-only"""
    if not text or not text.strip():
        return None
    
    original_text = text.strip()
    text = original_text
    
    # CRITICAL: Remove any Hindi/Devanagari characters that might leak through
    # This ensures TTS only receives pure English text
    # Remove Devanagari script characters (Hindi) - Unicode range U+0900 to U+097F
    hindi_chars = re.findall(r'[\u0900-\u097F]+', text)
    if hindi_chars:
        LOGGERS['tts'].error(f"‚ö†Ô∏è HINDI DETECTED IN TTS INPUT - Removing: '{''.join(hindi_chars)}' from text: '{text[:100]}...'")
    text = re.sub(r'[\u0900-\u097F]+', '', text)
    
    # Remove other non-ASCII characters except common punctuation and English letters
    # Keep: A-Z, a-z, 0-9, space, and common punctuation
    non_ascii_chars = re.findall(r'[^\x00-\x7F\s]', text)
    if non_ascii_chars:
        LOGGERS['tts'].warning(f"‚ö†Ô∏è Non-ASCII characters detected - Removing: '{''.join(set(non_ascii_chars))}' from text")
    text = re.sub(r'[^\x00-\x7F\s]', '', text)
    
    # Remove commas and full stops (user requirement) but preserve natural pauses
    text = text.replace(',', '').replace('.', '')
    
    # Validation: Ensure text has actual content (not just punctuation)
    text_no_punct = text.replace('!', '').replace('?', '').replace('-', '').replace(':', '').replace(';', '').strip()
    if not text_no_punct or len(text_no_punct) < 1:
        LOGGERS['tts'].warning(f"TTS text contains no meaningful content after sanitization - skipping: '{original_text[:50]}...'")
        return None
    
    # Quality check: Ensure meaningful words (not just single characters)
    words = text_no_punct.split()
    if len(words) == 0:
        LOGGERS['tts'].warning(f"TTS text contains no words after sanitization - skipping: '{original_text[:50]}...'")
        return None
    
    # Final validation: Ensure text contains English characters (A-Z, a-z) OR numbers (0-9)
    # Numbers are valid TTS input - TTS can speak "49" as "forty-nine" or "four nine"
    # Allow text with either letters or numbers (or both)
    has_letters = bool(re.search(r'[A-Za-z]', text))
    has_numbers = bool(re.search(r'[0-9]', text))
    
    if not has_letters and not has_numbers:
        LOGGERS['tts'].warning(f"TTS text contains no English letters or numbers - skipping: '{original_text[:50]}...'")
        return None
    
    # Preserve natural emphasis markers (exclamation, question marks for TTS intonation)
    # These help TTS understand emotion and emphasis
    
    return text


def sanitize_llm_input(text: str) -> str:
    """Sanitize LLM input to prevent injection attacks and ensure valid content"""
    if not text:
        return ""
    
    # Remove excessive newlines (potential injection vector)
    text = text.replace('\n\n\n', '\n').replace('\r\n', '\n')
    
    # Limit length to prevent abuse (2000 chars max)
    if len(text) > 2000:
        text = text[:2000] + "..."
    
    return text.strip()


# ============================================================================
# TEAM 1: THE EARS (listen_loop)
# Listens to Deepgram, throws text onto Conveyor Belt A (translation_queue)
# NEVER STOPS - Continuous listening
# ============================================================================
async def listen_loop(room: rtc.Room, translation_queue: asyncio.Queue, 
                    audio_amplitude_queue: asyncio.Queue) -> None:
    """Team 1: Ears - Continuous listening with prosody tracking"""
    audio_stream = None
    stt_stream = None
    
    try:
        def on_track_subscribed(track, publication, participant):
            nonlocal audio_stream
            if isinstance(track, rtc.RemoteAudioTrack) and not audio_stream:
                try:
                    audio_stream = rtc.AudioStream.from_track(
                        track=track, sample_rate=16000, num_channels=1
                    )
                except Exception as e:
                    pass
        
        room.on("track_subscribed", on_track_subscribed)
        
        # Check existing tracks
        for p in room.remote_participants.values():
            for pub in p.track_publications.values():
                if pub.track and isinstance(pub.track, rtc.RemoteAudioTrack):
                    try:
                        audio_stream = rtc.AudioStream.from_track(
                            track=pub.track, sample_rate=16000, num_channels=1
                        )
                        break
                    except Exception:
                        continue
                if audio_stream:
                    break
        
        # Wait for audio stream (30s timeout)
        waited = 0
        while not audio_stream and waited < 30:
            await asyncio.sleep(0.5)
            waited += 0.5
        
        if not audio_stream:
            return
    
        # Minimal delay for stability
        await asyncio.sleep(0.05)
        
        # Create STT stream with retry logic (3 attempts)
        max_stt_retries = 3
        stt_stream = None
        for attempt in range(max_stt_retries):
            try:
                LOGGERS['stt'].info(f"Initializing Deepgram STT stream - Attempt {attempt + 1}/{max_stt_retries}")
                LOGGERS['stt'].info(f"Deepgram config - Model: nova-3, Language: hi, Endpointing: {STT_ENDPOINTING_MS}ms, VAD: enabled")
                stt_stream = DEEPGRAM_STT.stream()
                LOGGERS['stt'].info("Deepgram STT stream initialized successfully")
                break
            except Exception as e:
                LOGGERS['error'].error(f"Deepgram STT initialization failed (attempt {attempt + 1}): {str(e)}")
                if attempt < max_stt_retries - 1:
                    await asyncio.sleep(0.1 * (2 ** attempt))  # Exponential backoff
                    continue
                LOGGERS['error'].error("Deepgram STT initialization failed after all retries")
                return
        
        if not stt_stream:
            return
        
        async def feed_audio():
            """Feed audio to STT and track prosody - runs until explicitly cancelled"""
            nonlocal audio_stream  # Must be declared before use
            while True:  # Keep running until cancelled, even if stream ends
                try:
                    if not audio_stream:
                        # Wait for audio stream to be available
                        await asyncio.sleep(0.5)
                        continue
                    
                    try:
                        async for event in audio_stream:
                            if event and event.frame and stt_stream:
                                try:
                                    stt_stream.push_frame(event.frame)
                                    
                                    # Prosody tracking: Calculate RMS for volume matching
                                    try:
                                        audio_data = np.frombuffer(event.frame.data, dtype=np.int16)
                                        if len(audio_data) > 0:
                                            rms = np.sqrt(np.mean(audio_data.astype(np.float32) ** 2))
                                            normalized_rms = min(rms / 32768.0, 1.0)
                                            db = 20.0 * np.log10(normalized_rms + 1e-10)
                                            
                                            try:
                                                audio_amplitude_queue.put_nowait((time.time(), db))
                                            except asyncio.QueueFull:
                                                # FIFO drop: Remove oldest amplitude reading
                                                try:
                                                    audio_amplitude_queue.get_nowait()
                                                    # Note: audio_amplitude_queue doesn't use task_done()
                                                    # It's a simple data queue, not a work queue
                                                    audio_amplitude_queue.put_nowait((time.time(), db))
                                                except asyncio.QueueEmpty:
                                                    # Queue became empty - try put again
                                                    try:
                                                        audio_amplitude_queue.put_nowait((time.time(), db))
                                                    except asyncio.QueueFull:
                                                        pass  # Drop this reading
                                    except Exception:
                                        pass  # Don't break audio feed on prosody errors
                                except (asyncio.CancelledError, Exception):
                                    break
                    except StopAsyncIteration:
                        # Stream ended - wait and try to reconnect
                        LOGGERS['stt'].warning("Audio stream ended - waiting for reconnection...")
                        await asyncio.sleep(1.0)
                        # Try to get new audio stream
                        for p in room.remote_participants.values():
                            for pub in p.track_publications.values():
                                if pub.track and isinstance(pub.track, rtc.RemoteAudioTrack):
                                    try:
                                        audio_stream = rtc.AudioStream.from_track(
                                            track=pub.track, sample_rate=16000, num_channels=1
                                        )
                                        LOGGERS['stt'].info("Audio stream reconnected")
                                        break
                                    except Exception:
                                        continue
                                if audio_stream:
                                    break
                        if not audio_stream:
                            await asyncio.sleep(2.0)  # Wait before retrying
                        continue
                except asyncio.CancelledError:
                    raise
                except Exception as e:
                    LOGGERS['error'].error(f"Audio feed error: {str(e)} - retrying...")
                    await asyncio.sleep(1.0)
                    continue
        
        async def process_transcripts():
            """Gear Shift Strategy: Fast Start (<3s) then Smooth Cruise with Reconnection Logic"""
            # Reconnection configuration - Optimized for faster recovery
            max_reconnect_attempts = 5  # Maximum reconnection attempts
            reconnect_delay_base = 0.5  # Base delay in seconds (reduced from 1.0s for faster recovery)
            max_reconnect_delay = 10.0  # Maximum delay between reconnections (reduced from 30.0s)
            reconnect_backoff_multiplier = 1.5  # Exponential backoff multiplier (gentler than 2.0)
            
            reconnect_count = 0
            nonlocal stt_stream
            
            while True:
                try:
                    speech_start_time = None
                    last_vad_event_time = None  # Track last START_OF_SPEECH event time for debouncing
                    transcript_count = 0
                    MAX_QUEUE_BACKPRESSURE = 8  # Don't send if queue has more than 8 items
                    deepgram_first_byte_time = None  # Track when Deepgram starts processing (for actual processing time)
                    
                    # Deduplication: Track recently sent texts to prevent duplicates
                    recent_sent_texts = []  # FIFO queue of last sent texts
                    MAX_RECENT_TEXTS = 5  # Keep last 5 sent texts for comparison
                    SIMILARITY_THRESHOLD = 0.75  # 75% word overlap = duplicate
                    
                    def calculate_similarity(text1: str, text2: str) -> float:
                        """Calculate word overlap similarity between two texts"""
                        words1 = set(text1.lower().strip().split())
                        words2 = set(text2.lower().strip().split())
                        
                        if len(words1) == 0 or len(words2) == 0:
                            return 0.0
                        
                        # Calculate Jaccard similarity (intersection over union)
                        intersection = len(words1 & words2)
                        union = len(words1 | words2)
                        return intersection / union if union > 0 else 0.0
                    
                    def is_duplicate_or_similar(text: str) -> bool:
                        """Check if text is duplicate or very similar to recently sent texts (final transcripts only)"""
                        text_normalized = text.lower().strip()
                        text_words = text_normalized.split()
                        
                        # Check all recent texts for duplicates
                        for sent_text in recent_sent_texts:
                            sent_normalized = sent_text.lower().strip()
                            sent_words = sent_normalized.split()
                            
                            # Exact match
                            if text_normalized == sent_normalized:
                                return True
                            
                            # Substring match - one contains the other
                            if text_normalized in sent_normalized or sent_normalized in text_normalized:
                                # Require 70% length similarity for final transcripts
                                len_ratio = min(len(text_normalized), len(sent_normalized)) / max(len(text_normalized), len(sent_normalized))
                                if len_ratio > 0.7:
                                    return True
                            
                            # Word overlap similarity
                            similarity = calculate_similarity(text, sent_text)
                            if similarity >= SIMILARITY_THRESHOLD:
                                return True
                        
                        return False
                    
                    def add_to_recent(text: str):
                        """Add text to recent sent texts (FIFO)"""
                        recent_sent_texts.append(text)
                        if len(recent_sent_texts) > MAX_RECENT_TEXTS:
                            recent_sent_texts.pop(0)
                    
                    # Reset reconnect count on successful connection
                    if reconnect_count > 0:
                        LOGGERS['stt'].info(f"STT stream reconnected successfully after {reconnect_count} attempts")
                        reconnect_count = 0
                    
                    # Ultra-Low Latency: No buffering, stream immediately
                    
                    try:
                        async for event in stt_stream:
                            if event.type == stt.SpeechEventType.START_OF_SPEECH:
                                # Debounce START_OF_SPEECH events to prevent multiple updates from rapid VAD triggers
                                current_time = time.time()
                                
                                # Debouncing: Ignore START_OF_SPEECH events within STT_VAD_DEBOUNCE_SEC of previous
                                if last_vad_event_time is not None:
                                    time_since_last_vad = current_time - last_vad_event_time
                                    if time_since_last_vad < STT_VAD_DEBOUNCE_SEC:
                                        # Ignore this VAD event - too soon after previous one
                                        LOGGERS['stt'].debug(f"Ignoring START_OF_SPEECH (debounced: {time_since_last_vad:.2f}s < {STT_VAD_DEBOUNCE_SEC}s)")
                                        continue
                                
                                # Update last VAD event time
                                last_vad_event_time = current_time
                                
                                # Set or update speech_start_time, but cap max age to prevent incorrect latency
                                if not speech_start_time:
                                    # First speech start - set timestamp
                                    speech_start_time = current_time
                                    deepgram_first_byte_time = current_time  # Track when Deepgram starts processing
                                    LOGGERS['stt'].debug(f"First START_OF_SPEECH - speech_start_time set to {current_time:.3f}")
                                elif (current_time - speech_start_time) > STT_SPEECH_START_MAX_AGE:
                                    # Speech start time is too old - reset it
                                    LOGGERS['stt'].debug(f"Resetting stale speech_start_time (age: {current_time - speech_start_time:.1f}s > {STT_SPEECH_START_MAX_AGE}s)")
                                    speech_start_time = current_time
                                    deepgram_first_byte_time = current_time
                                else:
                                    # Keep existing speech_start_time - this is continuation of same segment
                                    LOGGERS['stt'].debug(f"START_OF_SPEECH within same segment - keeping existing timestamp")
                                
                                LOGGERS['stt'].debug("Deepgram START_OF_SPEECH detected (VAD event)")
                            elif event.type == stt.SpeechEventType.INTERIM_TRANSCRIPT:
                                # DISABLED: Interim transcripts disabled to avoid duplicate translations
                                # Only processing final transcripts for reliability
                                continue
                            elif event.type == stt.SpeechEventType.FINAL_TRANSCRIPT:
                                if event.alternatives and event.alternatives[0].text.strip():
                                    text = event.alternatives[0].text.strip()
                                    transcript_count += 1
                                    
                                    # STEP 4: THE EARS NOISE GATE - Filter garbage transcripts
                                    # Filter empty text
                                    if not text or len(text) == 0:
                                        LOGGERS['stt'].debug("Filtered: Empty transcript")
                                        continue
                                    
                                    # Filter very short text (unless it's "ok" or "ji")
                                    text_lower = text.lower().strip()
                                    if len(text) < 2 and text_lower not in ["ok", "ji"]:
                                        LOGGERS['stt'].debug(f"Filtered: Too short - '{text}'")
                                        continue
                                    
                                    # Filter text containing only punctuation
                                    text_no_punct = text.replace('.', '').replace(',', '').replace('!', '').replace('?', '').replace('-', '').replace(':', '').replace(';', '').replace(' ', '').strip()
                                    if len(text_no_punct) == 0:
                                        LOGGERS['stt'].debug(f"Filtered: Only punctuation - '{text}'")
                                        continue
                                    
                                    # Log transcript (processing time will be added later if available)
                                    LOGGERS['stt'].info(f"Deepgram transcript #{transcript_count} - Text: '{text[:100]}...'")
                                    
                                    # Ultra-Low Latency: Stream immediately (no buffering)
                                    words = text.split()
                                    word_count = len(words)
                                    
                                    if word_count == 0:
                                        continue
                                    
                                    # Check minimum word threshold
                                    if word_count < STT_MIN_WORDS_THRESHOLD:
                                        LOGGERS['stt'].debug(f"Skipping: {word_count} words < minimum threshold ({STT_MIN_WORDS_THRESHOLD})")
                                        continue
                                    
                                    # Calculate latency
                                    transcript_received_time = time.time()
                                    deepgram_processing_time = None
                                    stt_latency = 0
                                    
                                    if speech_start_time:
                                        age = transcript_received_time - speech_start_time
                                        if age > STT_SPEECH_START_MAX_AGE:
                                            LOGGERS['stt'].debug(f"Resetting stale speech_start_time (age: {age:.1f}s)")
                                            speech_start_time = transcript_received_time
                                            deepgram_first_byte_time = transcript_received_time
                                        else:
                                            stt_latency = int(age * 1000)
                                            # Calculate actual Deepgram processing time
                                            if deepgram_first_byte_time:
                                                processing_time = transcript_received_time - deepgram_first_byte_time
                                                deepgram_processing_time = int(processing_time * 1000)
                                    
                                    # Stream immediately - no Gear logic, no buffering
                                    sanitized = sanitize_llm_input(text)
                                    if sanitized:
                                        # Check for duplicates
                                        if is_duplicate_or_similar(sanitized):
                                            LOGGERS['stt'].info(f"Skipping duplicate: '{sanitized[:50]}...'")
                                            continue
                                        
                                        # Check queue backpressure
                                        queue_size = translation_queue.qsize()
                                        if queue_size > MAX_QUEUE_BACKPRESSURE:
                                            LOGGERS['stt'].warning(f"Skipping - Queue too full: {queue_size}")
                                            continue
                                        
                                        LOGGERS['stt'].info(f"Streaming immediately - {word_count} words")
                                        LOGGERS['queue'].info(f"Queueing translation - Text: '{sanitized[:100]}...', Length: {len(sanitized)} chars")
                                        
                                        try:
                                            translation_queue.put_nowait(sanitized)
                                            add_to_recent(sanitized)
                                            LOGGERS['queue'].debug(f"Translation queue size: {translation_queue.qsize()}")
                                            
                                            if stt_latency > 0:
                                                latency_msg = f"T1_STT: {stt_latency}ms"
                                                if deepgram_processing_time:
                                                    latency_msg += f" (processing: {deepgram_processing_time}ms)"
                                                print(latency_msg)
                                                LOGGERS['perf'].info(f"T1_STT latency: {stt_latency}ms" + (f" (Deepgram processing: {deepgram_processing_time}ms)" if deepgram_processing_time else ""))
                                                perf_metrics.add_stt_latency(stt_latency)
                                        except asyncio.QueueFull:
                                            # FIFO drop
                                            try:
                                                dropped = translation_queue.get_nowait()
                                                translation_queue.task_done()
                                                LOGGERS['queue'].warning(f"Translation queue full - Dropped: '{str(dropped)[:50]}...'")
                                                translation_queue.put_nowait(sanitized)
                                                add_to_recent(sanitized)
                                            except asyncio.QueueEmpty:
                                                try:
                                                    translation_queue.put_nowait(sanitized)
                                                    add_to_recent(sanitized)
                                                except asyncio.QueueFull:
                                                    LOGGERS['queue'].error("Translation queue still full after drop attempt")
                                                    pass
                                    
                                    # Reset speech tracking after sending
                                    speech_start_time = None
                                    deepgram_first_byte_time = None
                    
                    except asyncio.CancelledError:
                        LOGGERS['stt'].info("STT transcript processing cancelled")
                        raise
                    except StopAsyncIteration:
                        # Stream ended normally - attempt reconnection
                        LOGGERS['stt'].warning("STT stream ended unexpectedly - attempting reconnection")
                    except (ConnectionError, OSError, RuntimeError) as conn_error:
                        # Connection-related errors - attempt reconnection
                        error_msg = str(conn_error)
                        LOGGERS['stt'].warning(f"STT connection error: {error_msg} - attempting reconnection")
                    except Exception as e:
                        error_str = str(e).lower()
                        error_msg = str(e)
                        # Check if it's a connection-related error
                        is_connection_error = any(keyword in error_str for keyword in [
                            'connection', 'websocket', 'network', 'timeout', 'closed', 
                            'broken pipe', 'reset by peer', 'connection refused'
                        ])
                        
                        if is_connection_error:
                            LOGGERS['stt'].warning(f"STT connection issue detected: {error_msg} - attempting reconnection")
                        else:
                            # Filter known harmless errors
                            if "timeout context manager" in error_str:
                                LOGGERS['stt'].debug("Known asyncio timeout issue - ignoring")
                            else:
                                LOGGERS['error'].error(f"STT transcript processing error: {error_msg}")
                
                except asyncio.CancelledError:
                    # Propagate cancellation to outer scope
                    raise
                finally:
                    # Reconnection logic (runs after any exception)
                    pass
                
                # Reconnection logic
                if reconnect_count < max_reconnect_attempts:
                    reconnect_count += 1
                    # Calculate exponential backoff delay
                    reconnect_delay = min(
                        reconnect_delay_base * (reconnect_backoff_multiplier ** (reconnect_count - 1)),
                        max_reconnect_delay
                    )
                    
                    LOGGERS['stt'].info(
                        f"Attempting STT stream reconnection in {reconnect_delay:.1f}s "
                        f"(Attempt {reconnect_count}/{max_reconnect_attempts})"
                    )
                    await asyncio.sleep(reconnect_delay)
                    
                    try:
                        # Close old stream if it exists
                        if stt_stream:
                            try:
                                await asyncio.wait_for(stt_stream.aclose(), timeout=1.0)
                            except Exception:
                                pass  # Ignore errors when closing old stream
                        
                        # Create new STT stream
                        LOGGERS['stt'].info(f"Reconnecting Deepgram STT stream - Attempt {reconnect_count}/{max_reconnect_attempts}")
                        stt_stream = DEEPGRAM_STT.stream()
                        LOGGERS['stt'].info("STT stream reconnected successfully - resuming processing")
                        
                        # Continue with new stream (will reset state in next iteration)
                        continue
                    except Exception as reconnect_error:
                        LOGGERS['error'].error(f"STT reconnection failed (attempt {reconnect_count}): {str(reconnect_error)}")
                        if reconnect_count >= max_reconnect_attempts:
                            LOGGERS['error'].error("STT reconnection failed after all attempts - stopping")
                            break
                        # Continue to next reconnection attempt
                        continue
                else:
                    LOGGERS['error'].error("STT reconnection exhausted all attempts - stopping transcript processing")
                    break
        await asyncio.gather(feed_audio(), process_transcripts(), return_exceptions=True)
    except asyncio.CancelledError:
        LOGGERS['stt'].info("Listen loop cancelled")
        pass
    except Exception as e:
        LOGGERS['error'].error(f"Listen loop error: {str(e)}")
        pass
    finally:
        if stt_stream:
            try:
                LOGGERS['stt'].info("Closing Deepgram STT stream...")
                # Safe stream closure with timeout
                close_task = asyncio.create_task(stt_stream.aclose())
                try:
                    await asyncio.wait_for(close_task, timeout=0.5)
                    LOGGERS['stt'].info("Deepgram STT stream closed successfully")
                except asyncio.TimeoutError:
                    if not close_task.done():
                        close_task.cancel()
                        try:
                            await close_task
                        except Exception:
                            pass
                    LOGGERS['stt'].warning("Deepgram STT stream close timeout")
            except Exception as e:
                LOGGERS['error'].error(f"Error closing STT stream: {str(e)}")
                pass  # Ignore closure errors


# ============================================================================
# TEAM 2: THE BRAIN (translate_and_synthesize_loop)
# Picks text from Belt A, Translates, generates Audio Frames, 
# throws Audio Frames onto Conveyor Belt B (audio_queue)
# RUNS IN PARALLEL - Never blocks
# ============================================================================

# Prosody cache (performance optimization)
# Thread-safe: All access is from async functions in same event loop (single-threaded)
_prosody_cache = {"timestamp": 0, "speed": 1.0, "volume": 1.0}


def get_prosody_from_amplitude(audio_amplitude_queue: asyncio.Queue) -> tuple[float, float]:
    """Get prosody from recent audio (cached for performance)"""
    current_time = time.time()
    
    # Check cache (reduces O(n) overhead)
    if current_time - _prosody_cache["timestamp"] < PROSODY_CACHE_TTL:
        return _prosody_cache["speed"], _prosody_cache["volume"]
    
    amplitudes = []
    temp_items = []
    
    # Collect recent amplitudes (non-destructive read)
    while True:
        try:
            timestamp, db = audio_amplitude_queue.get_nowait()
            if current_time - timestamp <= PROSODY_ANALYSIS_WINDOW:
                amplitudes.append(db)
                temp_items.append((timestamp, db))
        except asyncio.QueueEmpty:
            break
    
    # Put items back (non-destructive)
    for item in temp_items:
        try:
            audio_amplitude_queue.put_nowait(item)
        except asyncio.QueueFull:
            # FIFO drop: Remove oldest item to make room
            try:
                audio_amplitude_queue.get_nowait()
                # Note: audio_amplitude_queue doesn't use task_done()
                # It's a simple data queue, not a work queue
                audio_amplitude_queue.put_nowait(item)
            except asyncio.QueueEmpty:
                # Queue became empty - try put again
                try:
                    audio_amplitude_queue.put_nowait(item)
                except asyncio.QueueFull:
                    pass  # Drop this item
    
    if not amplitudes:
        _prosody_cache.update({"timestamp": current_time, "speed": 1.0, "volume": 1.0})
        return 1.0, 1.0
    
    avg_db = np.mean(amplitudes)
    max_db = np.max(amplitudes)
    
    # Map dB to speed (loud = faster)
    if avg_db > -20:
        speed = 1.15
    elif avg_db > -30:
        speed = 1.05
    elif avg_db > -40:
        speed = 1.0
    else:
        speed = 0.95
    
    # Map dB to volume (for emphasis)
    if max_db > -15:
        volume_boost = 1.2
    elif max_db > -25:
        volume_boost = 1.1
    else:
        volume_boost = 1.0
    
    _prosody_cache.update({"timestamp": current_time, "speed": speed, "volume": volume_boost})
    
    return speed, volume_boost


async def process_single_translation(hindi_text: str, chat_ctx: llm.ChatContext,
                                    audio_queue: asyncio.Queue,
                                    audio_amplitude_queue: asyncio.Queue,
                                    tts_semaphore: asyncio.Semaphore,
                                    shutdown_event: asyncio.Event,
                                    translation_lock: asyncio.Lock) -> str:
    """Process one translation with continuous streaming TTS - Water Hose method for prosody continuity"""
    # CRITICAL FIX: Add user message inside lock to prevent race conditions
    async with translation_lock:
        chat_ctx.add_message(role="user", content=hindi_text)
    
    trans_start = time.time()
    input_length = len(hindi_text)
    
    LOGGERS['llm'].info(f"Translation started - Hindi text: '{hindi_text[:100]}...' ({input_length} chars)")
    
    full_translation = ""
    tts_start_time = None
    
    try:
        # Open ONE persistent TTS stream at the start (Water Hose method)
        # This maintains prosody continuity across the entire response
        async with tts_semaphore:
            LOGGERS['tts'].info(f"Cartesia TTS stream opening - Voice: {CARTESIA_VOICE_ID}, Model: {CARTESIA_TTS_MODEL}, Sample Rate: 24kHz")
            async with CARTESIA_TTS.stream() as tts_stream:
                LOGGERS['tts'].debug("Cartesia TTS stream opened successfully")
                # Background task to consume audio from the stream
                async def consume_audio():
                    nonlocal tts_start_time
                    tts_start_time = time.time()
                    audio_frame_count = 0
                    total_audio_bytes = 0
                    try:
                        async for audio in tts_stream:
                            if shutdown_event.is_set():
                                LOGGERS['tts'].warning("TTS audio consumption stopped - shutdown event")
                                break
                            if audio.frame:
                                audio_frame_count += 1
                                frame_bytes = len(audio.frame.data) if hasattr(audio.frame, 'data') else 0
                                total_audio_bytes += frame_bytes
                                try:
                                    audio_queue.put_nowait(audio.frame)
                                    if audio_frame_count % 10 == 0:  # Log every 10th frame
                                        LOGGERS['tts'].debug(f"TTS audio frame #{audio_frame_count} - {frame_bytes} bytes, Queue size: {audio_queue.qsize()}")
                                except asyncio.QueueFull:
                                    # FIFO drop: Remove oldest frame to make room
                                    try:
                                        dropped_frame = audio_queue.get_nowait()
                                        audio_queue.task_done()
                                        LOGGERS['queue'].warning(f"Audio queue full - Dropped frame, Queue size: {audio_queue.qsize()}")
                                        audio_queue.put_nowait(audio.frame)
                                    except asyncio.QueueEmpty:
                                        try:
                                            audio_queue.put_nowait(audio.frame)
                                        except asyncio.QueueFull:
                                            LOGGERS['queue'].error("Audio queue still full after drop attempt")
                                            pass  # Drop this frame
                        elapsed = int((time.time() - tts_start_time) * 1000) if tts_start_time else 0
                        LOGGERS['tts'].info(f"TTS audio consumption completed - Frames: {audio_frame_count}, Total bytes: {total_audio_bytes}, Time: {elapsed}ms")
                    except Exception as e:
                        elapsed = int((time.time() - tts_start_time) * 1000) if tts_start_time else 0
                        LOGGERS['error'].error(f"TTS audio consumption error after {elapsed}ms: {str(e)}")
                        pass
                
                # Start audio consumption in background
                audio_task = asyncio.create_task(consume_audio())
                
                try:
                    # CRITICAL FIX: Context pruning is now handled in _build_messages() and process_with_context_update()
                    # No need for duplicate pruning here - it causes conflicts and race conditions
                    # The _build_messages() method already prunes to CONTEXT_MAX_MESSAGES (6 items)
                    
                    # Timeout protection: 4 seconds max per translation (with retry logic)
                    llm_chunk_count = 0
                    tts_text_chunks = 0
                    total_tts_chars = 0
                    translation_timeout_occurred = False
                    translation_success = False
                    
                    # CRITICAL FIX: Buffer for merging punctuation-only chunks with previous chunks
                    # This prevents punctuation from being filtered out during sanitization
                    tts_chunk_buffer = ""
                    
                    # Calculate adaptive timeout based on text length
                    adaptive_timeout = get_adaptive_translation_timeout(len(hindi_text))
                    
                    # Retry logic for translation timeouts with exponential backoff
                    for retry_attempt in range(MAX_TRANSLATION_RETRIES + 1):
                        if retry_attempt > 0:
                            # Exponential backoff: base_delay * (multiplier ^ (attempt - 1))
                            retry_delay = TRANSLATION_RETRY_DELAY_BASE * (TRANSLATION_RETRY_BACKOFF_MULTIPLIER ** (retry_attempt - 1))
                            LOGGERS['llm'].info(f"Retrying translation (attempt {retry_attempt + 1}/{MAX_TRANSLATION_RETRIES + 1}) after {retry_delay:.2f}s delay (exponential backoff)")
                            await asyncio.sleep(retry_delay)
                            trans_start = time.time()  # Reset timeout timer for retry
                            # Increase timeout on retry (network might be slow, allow more time for completion)
                            adaptive_timeout = min(adaptive_timeout * 1.3, 12.0)  # Increased cap to 12s for retries
                        
                        try:
                            translation_timeout_occurred = False  # Reset for each attempt
                            async with MAIN_LLM.chat(chat_ctx=chat_ctx) as llm_stream:
                                async for chunk in llm_stream:
                                    if shutdown_event.is_set():
                                        LOGGERS['llm'].warning("LLM streaming stopped - shutdown event")
                                        break
                                    
                                    # Check timeout with adaptive value - Optimized check (every 10 chunks to reduce overhead)
                                    elapsed_time = time.time() - trans_start
                                    if llm_chunk_count % 10 == 0 and elapsed_time > adaptive_timeout:
                                        # Only timeout if we haven't received a chunk recently (stream might be stalled)
                                        # Give extra grace period for stream completion (75% grace to prevent incomplete translations)
                                        if elapsed_time > adaptive_timeout * 1.75:  # 75% grace period (increased from 50%)
                                            if retry_attempt < MAX_TRANSLATION_RETRIES:
                                                LOGGERS['llm'].warning(f"Translation timeout after {elapsed_time:.1f}s (adaptive: {adaptive_timeout:.1f}s, text length: {len(hindi_text)} chars, will retry)")
                                                translation_timeout_occurred = True
                                                break
                                            else:
                                                LOGGERS['llm'].warning(f"Translation timeout after {elapsed_time:.1f}s (adaptive: {adaptive_timeout:.1f}s, text length: {len(hindi_text)} chars, max retries reached)")
                                                translation_timeout_occurred = True
                                                break
                                    
                                    text = ""
                                    if isinstance(chunk, str):
                                        text = chunk
                                    elif isinstance(chunk, llm.ChatChunk) and chunk.delta and chunk.delta.content:
                                        text = chunk.delta.content
                                    
                                    if text:
                                        llm_chunk_count += 1
                                        full_translation += text
                                        
                                        # CRITICAL: Check for Hindi in streaming output - detect early
                                        hindi_in_chunk = re.findall(r'[\u0900-\u097F]+', text)
                                        if hindi_in_chunk:
                                            LOGGERS['llm'].error(f"üö® CRITICAL: Hindi detected in LLM stream chunk: '{''.join(hindi_in_chunk)}' - Text: '{text[:100]}...'")
                                            # Don't send Hindi to TTS - skip this chunk
                                            continue
                                        
                                        # CRITICAL FIX: Buffer chunks to merge punctuation-only chunks with previous text
                                        # This prevents punctuation from being filtered out during sanitization
                                        tts_chunk_buffer += text
                                        
                                        # Check if current buffer contains meaningful content (not just punctuation)
                                        # Remove punctuation to check if there's actual text
                                        text_without_punct = re.sub(r'[^\w\s]', '', tts_chunk_buffer).strip()
                                        
                                        # If buffer has meaningful content OR buffer is getting large, process it
                                        # Process when: (1) has words, OR (2) buffer > 50 chars (prevent memory issues)
                                        if text_without_punct or len(tts_chunk_buffer) > 50:
                                            # Sanitize the buffered chunk (includes punctuation merged with text)
                                            sanitized = sanitize_text_for_tts(tts_chunk_buffer)
                                            if sanitized and len(sanitized) > 0:
                                                tts_text_chunks += 1
                                                total_tts_chars += len(sanitized)
                                                # Push buffered chunk to TTS
                                                tts_stream.push_text(sanitized)
                                                if tts_text_chunks % 5 == 0:  # Log every 5th chunk
                                                    LOGGERS['tts'].debug(f"TTS text pushed - Chunk #{tts_text_chunks}, Chars: {len(sanitized)}, Total: {total_tts_chars}")
                                                # Clear buffer after successful push
                                                tts_chunk_buffer = ""
                                            else:
                                                # If sanitization failed but buffer has content, it might be punctuation-only
                                                # Keep it in buffer to merge with next chunk
                                                if len(tts_chunk_buffer) > 50:
                                                    # Buffer too large - clear it to prevent memory issues
                                                    LOGGERS['tts'].warning(f"TTS buffer too large and sanitization failed - clearing: '{tts_chunk_buffer[:50]}...'")
                                                    tts_chunk_buffer = ""
                                        # If buffer is just punctuation, keep buffering to merge with next chunk
                            
                            # CRITICAL: Flush any remaining buffered chunks before ending
                            if tts_chunk_buffer:
                                sanitized = sanitize_text_for_tts(tts_chunk_buffer)
                                if sanitized and len(sanitized) > 0:
                                    tts_text_chunks += 1
                                    total_tts_chars += len(sanitized)
                                    tts_stream.push_text(sanitized)
                                    LOGGERS['tts'].debug(f"TTS text pushed (final flush) - Chunk #{tts_text_chunks}, Chars: {len(sanitized)}")
                                tts_chunk_buffer = ""  # Clear buffer
                            
                            # If we got here without timeout, translation succeeded
                            if not translation_timeout_occurred:
                                # Additional check: ensure we have a meaningful translation
                                if full_translation.strip() and len(full_translation.strip()) > 0:
                                    translation_success = True
                                    break  # Exit retry loop on success
                                else:
                                    LOGGERS['llm'].warning("Translation stream completed but result is empty - will retry")
                                    translation_timeout_occurred = True
                                    continue  # Retry if empty result
                            
                        except Exception as e:
                            if retry_attempt < MAX_TRANSLATION_RETRIES:
                                LOGGERS['error'].warning(f"Translation error (will retry): {str(e)}")
                                translation_timeout_occurred = True  # Mark for retry
                                continue  # Continue to next retry attempt
                            else:
                                LOGGERS['error'].error(f"Translation error (max retries reached): {str(e)}")
                                raise  # Re-raise on final attempt
                    
                    if translation_success:
                        LOGGERS['llm'].info(f"LLM streaming completed - Chunks: {llm_chunk_count}, Translation length: {len(full_translation)} chars")
                    else:
                        LOGGERS['llm'].warning(f"LLM streaming failed after {MAX_TRANSLATION_RETRIES + 1} attempts - Chunks: {llm_chunk_count}, Translation length: {len(full_translation)} chars")
                    LOGGERS['tts'].info(f"TTS text push completed - Chunks: {tts_text_chunks}, Total chars: {total_tts_chars}")
                    
                    # Mark end of input - stream will finish processing
                    tts_stream.end_input()
                    LOGGERS['tts'].debug("TTS stream end_input() called")
                    
                except asyncio.TimeoutError:
                    tts_stream.end_input()
                except Exception as e:
                    tts_stream.end_input()
                
                # Wait for audio consumption to complete
                try:
                    await asyncio.wait_for(audio_task, timeout=10.0)
                except asyncio.TimeoutError:
                    audio_task.cancel()
                    try:
                        await audio_task
                    except Exception:
                        pass
                
                # Log TTS timing
                if tts_start_time:
                    tts_elapsed = int((time.time() - tts_start_time) * 1000)
                    if tts_elapsed > 0:
                        print(f"T2_TTS: {tts_elapsed}ms")
                        LOGGERS['perf'].info(f"T2_TTS latency: {tts_elapsed}ms")
                        perf_metrics.add_tts_latency(tts_elapsed)
    
    except Exception as e:
        LOGGERS['error'].error(f"Translation processing error: {str(e)}")
        pass
    
    trans_end = time.time()
    trans_elapsed = int((trans_end - trans_start) * 1000)
    if trans_elapsed > 0:
        print(f"T2_TRANS: {trans_elapsed}ms")
        LOGGERS['perf'].info(f"T2_TRANS total latency: {trans_elapsed}ms")
        perf_metrics.add_total_latency(trans_elapsed)
    
    # Return cleaned translation with post-processing to remove incomplete markers
    cleaned_translation = full_translation.strip()
    
    # CRITICAL FIX: Detect and reject Hindi characters in translation output
    # If LLM outputs Hindi instead of English, this is a critical error
    hindi_chars_in_output = re.findall(r'[\u0900-\u097F]+', cleaned_translation)
    if hindi_chars_in_output:
        LOGGERS['llm'].error(f"üö® CRITICAL ERROR: LLM OUTPUT CONTAINS HINDI CHARACTERS: '{''.join(hindi_chars_in_output)}'")
        LOGGERS['llm'].error(f"üö® Translation output: '{cleaned_translation[:200]}...'")
        LOGGERS['llm'].error(f"üö® This indicates the LLM failed to translate to English - removing Hindi characters")
        # Remove Hindi characters as fallback
        cleaned_translation = re.sub(r'[\u0900-\u097F]+', '', cleaned_translation).strip()
        if len(cleaned_translation.strip()) < 3:
            LOGGERS['llm'].error(f"üö® Translation became empty after removing Hindi - this is a critical translation failure")
            return ""  # Return empty to trigger retry
    
    # CRITICAL FIX: Output validation - Detect context bleeding and quality issues
    # If output is >3x input length, it likely contains previous translations
    MAX_OUTPUT_EXPANSION = 3.0  # Allow up to 3x expansion (normal for translation)
    output_length = len(cleaned_translation)
    expansion_ratio = output_length / input_length if input_length > 0 else 0
    
    # Detect context bleeding (output contains previous translations)
    if expansion_ratio > MAX_OUTPUT_EXPANSION:
        LOGGERS['llm'].warning(f"‚ö†Ô∏è SUSPICIOUS OUTPUT EXPANSION: Input: {input_length} chars, Output: {output_length} chars (ratio: {expansion_ratio:.2f}x)")
        LOGGERS['llm'].warning(f"‚ö†Ô∏è This may indicate context bleeding - output may contain previous translations")
        # Try to extract only the last sentence (most recent translation)
        sentences = cleaned_translation.split('.')
        if len(sentences) > 1:
            # Take the last complete sentence (most recent translation)
            last_sentence = sentences[-2].strip() + '.' if len(sentences[-2].strip()) > 0 else sentences[-1].strip()
            if len(last_sentence) > 0 and len(last_sentence) <= input_length * MAX_OUTPUT_EXPANSION:
                cleaned_translation = last_sentence
                LOGGERS['llm'].info(f"‚ö†Ô∏è Extracted last sentence only: '{cleaned_translation[:100]}...' ({len(cleaned_translation)} chars)")
            else:
                # If last sentence is still too long, take first sentence only
                first_sentence = sentences[0].strip() + '.' if len(sentences[0].strip()) > 0 else cleaned_translation
                if len(first_sentence) <= input_length * MAX_OUTPUT_EXPANSION:
                    cleaned_translation = first_sentence
                    LOGGERS['llm'].info(f"‚ö†Ô∏è Extracted first sentence only: '{cleaned_translation[:100]}...' ({len(cleaned_translation)} chars)")
    
    # Quality check: Ensure translation is not empty or too short
    if len(cleaned_translation.strip()) < 3:
        LOGGERS['llm'].warning(f"‚ö†Ô∏è Translation too short ({len(cleaned_translation)} chars) - may be incomplete")
    
    # Remove trailing ellipsis that indicate incomplete translations
    # This handles cases where LLM generates "..." thinking it's continuing
    while cleaned_translation.endswith('...'):
        cleaned_translation = cleaned_translation[:-3].strip()
    
    # Remove trailing incomplete sentence markers
    if cleaned_translation.endswith('‚Ä¶'):
        cleaned_translation = cleaned_translation[:-1].strip()
    
    # Ensure translation ends with proper punctuation if it's a complete sentence
    if cleaned_translation and not cleaned_translation[-1] in '.!?':
        # Only add period if it looks like a complete sentence (has subject-verb structure)
        # Don't add if it's a fragment or list item
        if len(cleaned_translation.split()) > 3 and not cleaned_translation.endswith(','):
            cleaned_translation += '.'
    
    # Final validation: Check for incomplete translations
    if cleaned_translation.endswith('...') or cleaned_translation.endswith('‚Ä¶'):
        LOGGERS['llm'].warning(f"‚ö†Ô∏è Translation appears incomplete (ends with ellipsis): '{cleaned_translation[-50:]}'")
        # Try to remove the incomplete ending and add proper punctuation
        cleaned_translation = cleaned_translation.rstrip('‚Ä¶').rstrip('...').strip()
        if cleaned_translation and not cleaned_translation[-1] in '.!?':
            cleaned_translation += '.'
    
    # Additional quality check: Ensure translation doesn't contain multiple sentences that look like context bleeding
    sentences = cleaned_translation.split('.')
    if len(sentences) > 3 and expansion_ratio > 2.5:
        # If we have many sentences and high expansion, might be context bleeding
        # Take only the last complete sentence
        last_complete = sentences[-2].strip() + '.' if len(sentences) > 1 and len(sentences[-2].strip()) > 10 else cleaned_translation
        if len(last_complete) < len(cleaned_translation) * 0.6:  # If significantly shorter, use it
            LOGGERS['llm'].warning(f"‚ö†Ô∏è Multiple sentences detected with high expansion - using last sentence only")
            cleaned_translation = last_complete
    
    LOGGERS['llm'].info(f"Translation completed - Result: '{cleaned_translation[:100]}{'...' if len(cleaned_translation) > 100 else ''}' ({len(cleaned_translation)} chars, expansion: {expansion_ratio:.2f}x)")
    return cleaned_translation


async def translate_and_synthesize_loop(translation_queue: asyncio.Queue, 
                                    audio_queue: asyncio.Queue, 
                                    audio_amplitude_queue: asyncio.Queue, 
                                    shutdown_event: asyncio.Event) -> None:
    """Team 2: Brain - Sequential TTS with optimized pre-buffering for continuous flow"""
    chat_ctx = llm.ChatContext()
    chat_ctx.add_message(role="system", content=TRANSLATION_PROMPT)
    
    LOGGERS['llm'].info(f"Translation context initialized - System prompt: '{TRANSLATION_PROMPT[:100]}...'")
    
    tts_semaphore = asyncio.Semaphore(TTS_CONCURRENT_LIMIT)
    active_translations = set()
    translation_lock = asyncio.Lock()  # For thread-safe context updates
    translation_count = 0
    
    LOGGERS['main'].info(f"Translation loop ready - TTS concurrent limit: {TTS_CONCURRENT_LIMIT}")
    
    try:
        while not shutdown_event.is_set():
            try:
                # Get next translation (non-blocking with timeout)
                try:
                    hindi_text = await asyncio.wait_for(
                        translation_queue.get(), 
                        timeout=TRANSLATION_QUEUE_TIMEOUT
                    )
                except asyncio.TimeoutError:
                    continue
                
                if hindi_text is None:
                    LOGGERS['queue'].info("Received shutdown signal in translation queue")
                    break
                
                translation_count += 1
                LOGGERS['queue'].info(f"Translation #{translation_count} dequeued - Text: '{hindi_text[:100]}...'")
                
                # Process translation in parallel (don't wait for completion)
                async def process_with_context_update(text: str):
                    """Process translation and update context with quality validation"""
                    current_task = asyncio.current_task()
                    try:
                        full_translation = await process_single_translation(
                            text, chat_ctx, audio_queue, audio_amplitude_queue,
                            tts_semaphore, shutdown_event, translation_lock
                        )
                        
                        if full_translation:
                            # Quality validation: Ensure translation is meaningful
                            translation_words = full_translation.split()
                            if len(translation_words) > 0:
                                # Update context thread-safely
                                async with translation_lock:
                                    chat_ctx.add_message(role="assistant", content=full_translation)
                                    context_size_before = len(chat_ctx.items)
                                    # Smart context pruning: Keep system message + recent pairs
                                    if len(chat_ctx.items) > CONTEXT_MAX_ITEMS:
                                        # Find system message
                                        system_msg = None
                                        for item in chat_ctx.items:
                                            if item.type == "message" and item.role == "system":
                                                system_msg = item
                                                break
                                        
                                        if system_msg:
                                            # Keep system + recent conversation pairs
                                            recent_items = chat_ctx.items[-(CONTEXT_MAX_ITEMS - 1):]
                                            chat_ctx.items = [system_msg] + recent_items
                                            LOGGERS['llm'].debug(f"Context pruned - Before: {context_size_before}, After: {len(chat_ctx.items)}")
                                        else:
                                            # Fallback: keep first item + recent items
                                            if len(chat_ctx.items) > 0:
                                                first_item = chat_ctx.items[0]
                                                recent_items = chat_ctx.items[-(CONTEXT_MAX_ITEMS - 1):]
                                                chat_ctx.items = [first_item] + recent_items
                                                LOGGERS['llm'].debug(f"Context pruned (fallback) - Before: {context_size_before}, After: {len(chat_ctx.items)}")
                                    else:
                                        LOGGERS['llm'].debug(f"Context updated - Size: {len(chat_ctx.items)}")
                                
                    except Exception as e:
                        LOGGERS['error'].error(f"Translation processing error: {str(e)}")
                        pass
                    finally:
                        translation_queue.task_done()
                        if current_task:
                            active_translations.discard(current_task)
                
                # Start translation task (runs in parallel)
                task = asyncio.create_task(process_with_context_update(hindi_text))
                active_translations.add(task)
                
                # Cleanup completed translations (keep max 8 active for better quality)
                if len(active_translations) > 8:
                    active_translations = {t for t in active_translations if not t.done()}
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                try:
                    translation_queue.task_done()
                except ValueError:
                    pass
    finally:
        # Wait for active translations to complete
        if active_translations:
            await asyncio.gather(*active_translations, return_exceptions=True)


# ============================================================================
# TEAM 3: THE MOUTH (playback_loop)
# Picks audio frames from Conveyor Belt B and plays them
# STRICTLY PACED at 1.0x speed (normal speed)
# ============================================================================
async def playback_loop(source: rtc.AudioSource, audio_queue: asyncio.Queue, 
                    shutdown_event: asyncio.Event) -> None:
    """Team 3: Mouth - De-Clicker with Accumulator Buffer (Fixes robotic audio clicks)"""
    BASE_SPEED = 1.0  # Normal speed (1.0x)
    
    LOGGERS['main'].info("Playback loop started - De-Clicker mode with accumulator buffer")
    LOGGERS['main'].info(f"MIN_PLAYABLE_SIZE: {MIN_PLAYABLE_SIZE} bytes (~75ms audio - Optimized balance for low latency)")
    
    # STEP 3: THE MOUTH DE-CLICKER - Accumulator Buffer
    frame_buffer = bytearray()  # Accumulator buffer
    playback_count = 0
    
    try:
        while not shutdown_event.is_set():
            try:
                # Get frame with timeout - Optimized for faster pickup
                try:
                    audio_frame = await asyncio.wait_for(audio_queue.get(), timeout=0.03)
                except asyncio.TimeoutError:
                    # TIMEOUT HANDLING: If silence occurs, flush buffer if it has content
                    if len(frame_buffer) > 0:
                        LOGGERS['perf'].debug(f"Timeout - Flushing buffer: {len(frame_buffer)} bytes")
                        # Create AudioFrame from buffer
                        try:
                            buffered_frame = rtc.AudioFrame(
                                data=bytes(frame_buffer),
                                sample_rate=24000,
                                num_channels=1,
                                samples_per_channel=len(frame_buffer) // 2
                            )
                            playback_start = time.time()
                            await source.capture_frame(buffered_frame)
                            playback_end = time.time()
                            playback_elapsed = int((playback_end - playback_start) * 1000)
                            playback_count += 1
                            
                            # Calculate sleep time (16-bit, 24kHz, 1 channel)
                            buffer_duration = (len(frame_buffer) / 2) / 24000.0
                            sleep_time = min(buffer_duration / BASE_SPEED, 0.1)
                            await asyncio.sleep(sleep_time)
                            
                            frame_buffer.clear()
                            if playback_elapsed > 0:
                                LOGGERS['perf'].debug(f"T3_PLAY (flushed): {playback_elapsed}ms, Buffer: {len(frame_buffer)} bytes")
                        except Exception as e:
                            LOGGERS['error'].error(f"Error flushing buffer: {str(e)}")
                            frame_buffer.clear()
                    else:
                        # No buffer, just wait - Reduced sleep for faster response
                        await asyncio.sleep(0.005)
                    continue
                
                if audio_frame is None:
                    # Flush any remaining buffer before shutdown
                    if len(frame_buffer) > 0:
                        try:
                            buffered_frame = rtc.AudioFrame(
                                data=bytes(frame_buffer),
                                sample_rate=24000,
                                num_channels=1,
                                samples_per_channel=len(frame_buffer) // 2
                            )
                            await source.capture_frame(buffered_frame)
                            frame_buffer.clear()
                        except Exception:
                            pass
                    break
                
                # DON'T PLAY IMMEDIATELY - Add to accumulator buffer
                if hasattr(audio_frame, 'data') and audio_frame.data:
                    frame_buffer.extend(audio_frame.data)
                else:
                    # Fallback: try to get data another way
                    try:
                        frame_data = bytes(audio_frame)
                        frame_buffer.extend(frame_data)
                    except Exception:
                        LOGGERS['error'].warning("Could not extract audio data from frame")
                
                audio_queue.task_done()
                
                # Check if buffer is large enough to play (MIN_PLAYABLE_SIZE = 3600 bytes ~75ms - Optimized balance)
                if len(frame_buffer) >= MIN_PLAYABLE_SIZE:
                    # Create AudioFrame from accumulated buffer
                    try:
                        buffered_frame = rtc.AudioFrame(
                            data=bytes(frame_buffer),
                            sample_rate=24000,
                            num_channels=1,
                            samples_per_channel=len(frame_buffer) // 2
                        )
                        
                        playback_start = time.time()
                        await source.capture_frame(buffered_frame)
                        playback_end = time.time()
                        playback_elapsed = int((playback_end - playback_start) * 1000)
                        playback_count += 1
                        
                        # Calculate sleep time (16-bit, 24kHz, 1 channel)
                        buffer_duration = (len(frame_buffer) / 2) / 24000.0
                        sleep_time = min(buffer_duration / BASE_SPEED, 0.1)
                        
                        if playback_count % 20 == 0:  # Log every 20th playback
                            LOGGERS['perf'].debug(f"T3_PLAY #{playback_count} - Latency: {playback_elapsed}ms, Buffer: {len(frame_buffer)} bytes, Duration: {buffer_duration:.3f}s")
                        
                        # Clear buffer after playing
                        frame_buffer.clear()
                        
                        # Sleep for calculated duration
                        await asyncio.sleep(sleep_time)
                    except Exception as e:
                        LOGGERS['error'].error(f"Playback error: {str(e)}")
                        frame_buffer.clear()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                LOGGERS['error'].error(f"Playback loop error: {str(e)}")
                pass
    except asyncio.CancelledError:
        pass
    finally:
        # Final flush on shutdown
        if len(frame_buffer) > 0:
            try:
                buffered_frame = rtc.AudioFrame(
                    data=bytes(frame_buffer),
                    sample_rate=24000,
                    num_channels=1,
                    samples_per_channel=len(frame_buffer) // 2
                )
                await source.capture_frame(buffered_frame)
                LOGGERS['main'].info(f"Final buffer flush: {len(frame_buffer)} bytes")
            except Exception:
                pass


async def entrypoint(ctx: JobContext) -> None:
    """Main entrypoint - The Factory Line coordinator"""
    LOGGERS['main'].info("=" * 80)
    LOGGERS['main'].info("AGENT STARTING - Entrypoint called")
    LOGGERS['main'].info(f"Models configured:")
    LOGGERS['main'].info(f"  - STT: Deepgram Nova-3 (Hindi)")
    LOGGERS['main'].info(f"  - LLM: Cerebras/Groq (openai/gpt-oss-20b)")
    LOGGERS['main'].info(f"  - TTS: Cartesia ({CARTESIA_TTS_MODEL}, voice: {CARTESIA_VOICE_ID})")
    LOGGERS['main'].info("=" * 80)
    
    # Pre-warm connections to eliminate cold start latency
    LOGGERS['main'].info("Pre-warming connections...")
    try:
        # Pre-warm TTS connection
        LOGGERS['tts'].info("Pre-warming TTS connection...")
        CARTESIA_TTS.prewarm()
        LOGGERS['tts'].info("TTS connection pre-warmed successfully")
    except Exception as e:
        LOGGERS['tts'].warning(f"TTS pre-warming failed (non-critical): {str(e)}")
        # Continue anyway - pre-warming is optional
    
    # Pre-warm STT connection by creating a test stream
    try:
        LOGGERS['stt'].info("Pre-warming STT connection...")
        test_stt_stream = DEEPGRAM_STT.stream()
        await asyncio.sleep(0.1)  # Brief connection establishment
        try:
            await test_stt_stream.aclose()
        except Exception:
            pass
        LOGGERS['stt'].info("STT connection pre-warmed successfully")
    except Exception as e:
        LOGGERS['stt'].warning(f"STT pre-warming failed (non-critical): {str(e)}")
        # Continue anyway - pre-warming is optional
    
    # LLM connection already warmed via TCP warming in CerebrasLLMWrapper init
    LOGGERS['main'].info("LLM connection ready (TCP warmed during init)")
    
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    LOGGERS['main'].info("Connected to LiveKit room")
    
    room = ctx.room
    if not room:
        LOGGERS['error'].error("Room not available")
        return
    
    # Wait for local participant
    waited = 0
    while not room.local_participant and waited < 10:
        await asyncio.sleep(0.2)
        waited += 0.2
    
    local_participant = room.local_participant
    if not local_participant:
        LOGGERS['error'].error("Local participant not available")
        return
    
    LOGGERS['main'].info("Local participant ready")
    
    # Set up audio output (24kHz mono - standard for voice)
    source = rtc.AudioSource(24000, 1)
    track = rtc.LocalAudioTrack.create_audio_track("tts-output", source)
    options = rtc.TrackPublishOptions(source=rtc.TrackSource.SOURCE_MICROPHONE)
    
    try:
        await local_participant.publish_track(track, options)
        LOGGERS['main'].info("Audio track published - Sample rate: 24kHz, Channels: 1")
    except Exception as e:
        LOGGERS['error'].error(f"Failed to publish audio track: {str(e)}")
        return
    
    # Create queues (bounded to prevent memory leaks)
    # Optimized queue sizes for performance:
    # - Translation queue: 50 (handles burst of transcriptions)
    # - Audio queue: 50 (smooth playback buffer)
    # - Amplitude queue: 100 (prosody analysis needs more samples)
    translation_queue = asyncio.Queue(maxsize=50)
    audio_queue = asyncio.Queue(maxsize=50)
    audio_amplitude_queue = asyncio.Queue(maxsize=100)
    
    LOGGERS['queue'].info("Queues initialized - Translation: 50, Audio: 50, Amplitude: 100")
    LOGGERS['queue'].info("‚úì Queue optimization: Bounded queues prevent memory leaks and backpressure")
    
    shutdown_event = asyncio.Event()
    
    # Start all three teams with optimized task creation
    # Using create_task for immediate scheduling (non-blocking)
    LOGGERS['main'].info("Starting factory line teams...")
    
    # Team 1: Ears (STT) - Highest priority for real-time transcription
    team1_ears = asyncio.create_task(listen_loop(room, translation_queue, audio_amplitude_queue))
    LOGGERS['main'].info("‚úì Team 1 (Ears) started - Deepgram STT listening")
    
    # Team 2: Brain (LLM + TTS) - Parallel processing for translation and synthesis
    team2_brain = asyncio.create_task(translate_and_synthesize_loop(
        translation_queue, audio_queue, audio_amplitude_queue, shutdown_event
    ))
    LOGGERS['main'].info("‚úì Team 2 (Brain) started - LLM translation + TTS synthesis")
    
    # Team 3: Mouth (Playback) - Real-time audio output
    team3_mouth = asyncio.create_task(playback_loop(source, audio_queue, shutdown_event))
    LOGGERS['main'].info("‚úì Team 3 (Mouth) started - Audio playback")
    
    LOGGERS['main'].info("üöÄ All teams operational - Factory line running at full speed")
    
    # Start periodic performance summary logging (every 60 seconds)
    async def periodic_performance_log():
        """Periodically log performance metrics summary"""
        while not shutdown_event.is_set():
            try:
                await asyncio.sleep(60)  # Log every 60 seconds
                if not shutdown_event.is_set():
                    perf_metrics.log_summary()
            except asyncio.CancelledError:
                break
            except Exception as e:
                LOGGERS['perf'].debug(f"Performance logging error: {str(e)}")
                await asyncio.sleep(60)
    
    perf_log_task = asyncio.create_task(periodic_performance_log())
    
    # Run until cancelled
    try:
        LOGGERS['main'].info("Factory line running - Waiting for tasks...")
        results = await asyncio.gather(team1_ears, team2_brain, team3_mouth, return_exceptions=True)
        # Check if any task completed unexpectedly (not cancelled)
        for i, (task, result) in enumerate(zip([team1_ears, team2_brain, team3_mouth], results), 1):
            if isinstance(result, Exception) and not isinstance(result, asyncio.CancelledError):
                LOGGERS['error'].error(f"Team {i} task failed with exception: {str(result)}")
            elif not isinstance(result, Exception) and not isinstance(result, asyncio.CancelledError):
                LOGGERS['main'].warning(f"Team {i} task completed unexpectedly (not cancelled)")
    except Exception as e:
        LOGGERS['error'].error(f"Factory line error: {str(e)}")
        pass
    finally:
        # Cancel performance logging task
        perf_log_task.cancel()
        try:
            await perf_log_task
        except Exception:
            pass
        # Log final performance summary
        perf_metrics.log_summary()
        # Graceful shutdown
        LOGGERS['main'].info("Shutting down factory line...")
        shutdown_event.set()
        
        try:
            translation_queue.put_nowait(None)
            LOGGERS['queue'].debug("Sent shutdown signal to translation queue")
        except Exception:
            pass
        try:
            audio_queue.put_nowait(None)
            LOGGERS['queue'].debug("Sent shutdown signal to audio queue")
        except Exception:
            pass
        
        await asyncio.sleep(0.1)
        
        for task in [team1_ears, team2_brain, team3_mouth]:
            if not task.done():
                task.cancel()
        
        tasks_to_wait = [t for t in [team1_ears, team2_brain, team3_mouth] if not t.done()]
        if tasks_to_wait:
            try:
                await asyncio.wait_for(
                    asyncio.gather(*tasks_to_wait, return_exceptions=True),
                    timeout=1.0
                )
            except (asyncio.TimeoutError, Exception):
                for task in tasks_to_wait:
                    if not task.done():
                        try:
                            task.cancel()
                        except Exception:
                            pass
        
        LOGGERS['main'].info("=" * 80)
        LOGGERS['main'].info("AGENT SHUTDOWN COMPLETE")
        LOGGERS['main'].info(f"Log file: {LOGGERS['file']}")
        LOGGERS['main'].info("=" * 80)
        
        # Close log file handle if it exists
        if 'file_handle' in LOGGERS and LOGGERS['file_handle']:
            try:
                LOGGERS['file_handle'].flush()
                LOGGERS['file_handle'].close()
            except Exception:
                pass
        
        # Restore original stdout/stderr
        if hasattr(sys.stdout, 'console_stream'):
            sys.stdout = sys.stdout.console_stream
        if hasattr(sys.stderr, 'console_stream'):
            sys.stderr = sys.stderr.console_stream


# ============================================================================
# PERFORMANCE OPTIMIZATIONS - Platform-Specific Event Loop
# ============================================================================
# Event loop optimization is handled above:
# - Windows: winloop (significantly faster than default)
# - Linux/Mac: uvloop (2-4x faster)
# 
# Additional optimizations:
# 1. Efficient async operations with proper task scheduling
# 2. Reduced context switching through optimized coroutines
# 3. Optimized queue operations with bounded queues
# 4. Parallel processing with asyncio.gather()

def optimize_event_loop():
    """Verify event loop optimization is active"""
    try:
        current_policy = asyncio.get_event_loop_policy()
        policy_name = type(current_policy).__name__
        
        if sys.platform == 'win32':
            # Check if winloop policy is active
            if 'winloop' in policy_name.lower() or 'WinLoop' in policy_name:
                return True
        else:
            # Check if uvloop policy is active
            if 'uvloop' in policy_name.lower() or 'UVLoop' in policy_name:
                return True
        
        return False
    except Exception:
        return False

# Verify optimization is available (policy is set, will be used when event loop is created)
try:
    if sys.platform == 'win32':
        import winloop
        print("‚úì winloop available - Event loop will use winloop when created by LiveKit CLI")
    else:
        import uvloop
        print("‚úì uvloop available - Event loop will use uvloop when created by LiveKit CLI")
except ImportError:
    print("‚ö† Fast event loop not available - using default event loop")

if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
